{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bed343e-f373-46b8-a159-0ab87e2957be",
   "metadata": {},
   "source": [
    "# Lab No 3: Geovisualization Techniques - Data Viz - Part 1: 4 Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57784a3-e3a2-4999-b1fc-c5b46ae5e2f2",
   "metadata": {},
   "source": [
    "## Challenge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d65b65-2182-4e3f-8646-c60ec62fb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the csv file\n",
    "listings =pd.read_csv(\"/Users/hoitik/Desktop/Assginment_1/Lab 3/listings.csv\")\n",
    "\n",
    "# Create a subset to extract useful attribute\n",
    "subset_listings = listings[['id', \n",
    "                            'neighbourhood_cleansed',\n",
    "                            'latitude',\n",
    "                            'longitude',\n",
    "                            'property_type',\n",
    "                            'room_type',\n",
    "                            'bedrooms',\n",
    "                            'price',\n",
    "                            'number_of_reviews']]\n",
    "\n",
    "# Delete any \\$, in the price column, allows us to use .astype to make it numeric \n",
    "subset_listings.loc[subset_listings.index, 'price'] = subset_listings['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Drop any NaN value\n",
    "subset_listings = subset_listings.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a294a4-b82e-4d1a-aefe-a033cfc48051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function 'data_description' to produce the mean \n",
    "# and standard deviation for the numeric data frame\n",
    "\n",
    "\n",
    "def data_description(x): \n",
    "    # Select only the data with a numeric data types\n",
    "    numOnly_df = subset_listings.select_dtypes(include=['number'])\n",
    "\n",
    "    # Define stats as the outcome of the fuction\n",
    "    # which included the .mean and .count fuction\n",
    "    stats = pd.DataFrame({\n",
    "        \"Mean\": numOnly_df.mean(),\n",
    "        \"Counts\": numOnly_df.count()\n",
    "    })\n",
    "\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# plug in the subset and run the fuction\n",
    "result = data_description(subset_listings.price)\n",
    "\n",
    "# Display result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fc30a-0fc1-45a5-b1d0-7ecc614d8ae1",
   "metadata": {},
   "source": [
    "## Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd4118-71eb-45c0-b854-4157c73e1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# access the Qualified Property Sales record in New York\n",
    "# The API has a default limit of 1000 rows of data, \n",
    "# adding in ?$limit=1400 in the URL allows us to access all 1400 rows \n",
    "url_price = \"https://data.cityofnewyork.us/resource/8wi4-bsy4.json?$limit=1400\"   \n",
    "response_price = requests.get(url_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa38233-2085-4216-915c-3487495e497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convering the response to json and creates data frame \n",
    "data_price = response_price.json()\n",
    "price_pd = pd.DataFrame(data_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501367ad-d7a6-4ecf-b8a7-ca215538646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NaN vaule\n",
    "nan_in_column_Long = price_pd['longitude'].isna().any()\n",
    "nan_in_column_Lat = price_pd['latitude'].isna().any()\n",
    "\n",
    "#Display result\n",
    "print(nan_in_column_Long,nan_in_column_Lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f368-9756-412b-8d45-47a8bcd845b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN value\n",
    "clean_price_pd = price_pd.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca29d9-cdf1-4377-ae6c-f48b41269b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear unwanted attribute\n",
    "df_P = clean_price_pd[['latitude', 'longitude', 'price','cap_rate','council_district']]\n",
    "\n",
    "# Ensure price and cap_rate to be numeric\n",
    "df_P['price'] = df_P['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "df_P.loc[:, 'cap_rate'] = pd.to_numeric(df_P['cap_rate'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987d773-b149-4089-9b39-32b442f0a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# By using folium map\n",
    "# create a heatMap for the distribution of houses sales\n",
    "\n",
    "# Creating the Map and Making New York the central viewpoint\n",
    "m = folium.Map(location=[40.7128, -74.0060], zoom_start=10) \n",
    "\n",
    "\n",
    "heat_data = list(zip(df_P['latitude'], df_P['longitude']))\n",
    "HeatMap(heat_data, min_opacity=0.4, radius=10, blur=15).add_to(m)\n",
    "\n",
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ee482-e3f5-49f4-981d-225953baaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better illustrate house price\n",
    "# I found the shape file of NYC council district\n",
    "\n",
    "# Reading the Shape file\n",
    "districts_gdf = gpd.read_file('/Users/hoitik/Desktop/Assginment_1/Lab 3/nycc.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194125-8219-4b6f-95f1-43d578ddbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a geo data frame for the NYC house price data\n",
    "gdf_p = gpd.GeoDataFrame(df_P, \n",
    "                          geometry=gpd.points_from_xy(df_P['longitude'], df_P['latitude']),\n",
    "                          crs=\"EPSG:4326\")  \n",
    "\n",
    "# Making sure the two geo data frames use the same CRS\n",
    "gdf_p = gdf_p.to_crs(districts_gdf.crs)\n",
    "\n",
    "# Display the first five row \n",
    "gdf_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6f156-5aeb-4dae-b786-473255f00732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sjoin to combine the two geo data frames\n",
    "gdf_joined = gpd.sjoin(gdf_p, districts_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Create a new column with the average house price grouped by council district\n",
    "avg_price_per_district = gdf_joined.groupby('CounDist')['price'].mean().reset_index()\n",
    "\n",
    "# Merge the new column into the NYC council district map\n",
    "districts_gdf = districts_gdf.merge(avg_price_per_district, how='left', on='CounDist')\n",
    "\n",
    "# Display the first five row \n",
    "districts_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a8b7a-ddd0-451b-b367-1cc2982d20ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the Base Map and Making New York the central viewpoint\n",
    "m_P_1 = folium.Map(location=[40.7128, -74.0060], zoom_start=10)\n",
    "\n",
    "# Adding in the data\n",
    "folium.Choropleth(\n",
    "    geo_data=districts_gdf, # Using the merged map  \n",
    "    data=districts_gdf,\n",
    "    columns=['CounDist', 'price'],  \n",
    "    key_on='feature.properties.CounDist',  \n",
    "    fill_color='YlGnBu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average House Price in NYC by District',\n",
    "    bins=7,\n",
    "    reset=True\n",
    ").add_to(m_P_1)\n",
    "\n",
    "# Displaying the map\n",
    "m_P_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451039a7-6136-4b02-b796-7db0c6e75404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runing the mean, median and standard deviation for price and capitalization rate\n",
    "price_mean = df_P['price'].mean()\n",
    "price_median = df_P['price'].median()\n",
    "price_std = df_P['price'].std()\n",
    "\n",
    "cap_rate_mean = df_P['cap_rate'].mean()\n",
    "cap_rate_median = df_P['cap_rate'].median()\n",
    "cap_rate_std = df_P['cap_rate'].std()\n",
    "\n",
    "\n",
    "print(f\"Price - Mean: {price_mean:.2f}, Median: {price_median:.2f}, Std Dev: {price_std:.2f}\")\n",
    "print(f\"Cap Rate - Mean: {cap_rate_mean:.2f}, Median: {cap_rate_median:.2f}, Std Dev: {cap_rate_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6c102-287c-4a21-9c0f-2481b81ff613",
   "metadata": {},
   "source": [
    "The descriptive statistics result shows a high deviation of house prices in New York. \n",
    "The standard deviation for the house price and capitalization rate in New York is extremely high. Even our result from the Map of New York house prices by district shows that the house prices in Upper West Manhattan in comparison much higher than other districts in New York. It means that outliers might exist in the data, pulling the variance away from the mean. To gain a better understanding of the New York City housing situation, outliers should be removed and help normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e54fd-c0cd-4d8c-8002-1f7db5b6f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# To remove outliers from the capitalization rate, \n",
    "# we have to find out the first quantile and third quantile and \n",
    "# then keep the data between the two\n",
    "\n",
    "Q1_cap_rate = df_P['cap_rate'].quantile(0.25) \n",
    "Q3_cap_rate = df_P['cap_rate'].quantile(0.75)\n",
    "IQR_cap_rate = Q3_cap_rate - Q1_cap_rate\n",
    "\n",
    "# Define the outlier bounds by the Tukey Fences method\n",
    "lower_bound_cap_rate = Q1_cap_rate - 1.5 * IQR_cap_rate\n",
    "upper_bound_cap_rate = Q3_cap_rate + 1.5 * IQR_cap_rate\n",
    "\n",
    "# Now we only keep the capitalization rate falls between the 'lower_bound_cap_rate' and 'upper_bound_cap_rate'\n",
    "df_P_filtered = df_P[(df_P['cap_rate'] >= lower_bound_cap_rate) & (df_P['cap_rate'] <= upper_bound_cap_rate)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03674733-59d4-42c4-8dc0-ac60020c785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the same with the price\n",
    "\n",
    "Q1_price = df_P['price'].quantile(0.25)\n",
    "Q3_price = df_P['price'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "\n",
    "\n",
    "df_P_filtered = df_P[(df_P['cap_rate'] >= lower_bound_cap_rate) & (df_P['cap_rate'] <= upper_bound_cap_rate) &\n",
    "                     (df_P['price'] >= lower_bound_price) & (df_P['price'] <= upper_bound_price)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45cf32-7fa2-483e-9ca8-360eca79fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring both cap_rate and price are numeric\n",
    "df_P_filtered['cap_rate'] = pd.to_numeric(df_P_filtered['cap_rate'], errors='coerce')\n",
    "df_P_filtered['price'] = pd.to_numeric(df_P_filtered['price'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de47992-3221-45bf-8433-602a482eb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the descriptive statistic again\n",
    "price_mean = df_P_filtered['price'].mean()\n",
    "price_median = df_P_filtered['price'].median()\n",
    "price_std = df_P_filtered['price'].std()\n",
    "\n",
    "cap_rate_mean = df_P_filtered['cap_rate'].mean()\n",
    "cap_rate_median = df_P_filtered['cap_rate'].median()\n",
    "cap_rate_std = df_P_filtered['cap_rate'].std()\n",
    "\n",
    "\n",
    "print(f\"Price - Mean: {price_mean:.2f}, Median: {price_median:.2f}, Std Dev: {price_std:.2f}\")\n",
    "print(f\"Cap Rate - Mean: {cap_rate_mean:.2f}, Median: {cap_rate_median:.2f}, Std Dev: {cap_rate_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794d14b-ca72-46b6-a6c0-71728d4b3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Creating two subplots in a single figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))  \n",
    "\n",
    "# Plotting a histogram for the normalized capitalization rate\n",
    "sns.histplot(df_P_filtered['cap_rate'], bins=50, kde=True, color='blue', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Houses Capitalization Rate (Outliers Excluded)')\n",
    "axes[0].set_xlabel('Capitalization Rate')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting a histogram for the normalized New York Houses price\n",
    "sns.histplot(df_P_filtered['price'], bins=50, kde=True, color='green', ax=axes[1])\n",
    "axes[1].set_title('Distribution of House Prices (Outliers Excluded)')\n",
    "axes[1].set_xlabel('Price(USD10m)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Display figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3ca3e-284e-46df-b105-72707e319417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# With the normalized capitalization rate and normalized New York Houses price,\n",
    "# standard deviation for both variances shrank almost 10 times,  \n",
    "# We can now test for the correlation between the price and capitalization rate\n",
    "\n",
    "# Poltting a the scatter plot with regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(data=df_P_filtered, x='cap_rate', y='price', color='red', scatter_kws={'alpha': 0.5}, line_kws={'color': 'blue'})\n",
    "\n",
    "\n",
    "plt.title('Correlation between House Price and Capitalization Rate (Outliers Excluded)')\n",
    "plt.xlabel('Capitalization Rate')\n",
    "plt.ylabel('Price(USD10m)')\n",
    "\n",
    "# Display figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ba46b-6a5e-4f3d-b0a5-f88661a3061d",
   "metadata": {},
   "source": [
    "With the regression line in the graph, it is clear that house prices and Capitalization Rates are negatively correlated. The more expensive the houses, the lower the Capitalization rate. It shows that cheaper houses have higher profitability in investing, in comparison to the current US bank rate of 4.25%, the New York house capitalization Rate holds a mean of 5% and nearly 8% for houses priced at USD 5-6m. Prices for cheaper houses might increase as the demand rises for investment, especially for houses in Upper Manhattan and Washington Heights (where house sales are mostly distributed). Housing in New York City could be less affordable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbbecd-f183-4a42-9f37-87512a4f0355",
   "metadata": {},
   "source": [
    "## Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c4c77-9d65-4539-ae2e-f35c08367d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# access the API with a URL path, \n",
    "# Since the website has defaulted a limit of 1000 rows of data per entry\n",
    "# adding ?$limit= to the URL allows us to set the limit on our own\n",
    "# Now the limit is set as 2m, which is much closer to the actual dataset size\n",
    "\n",
    "url_collisions = \"https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit=2000000\"\n",
    "response = requests.get(url_collisions)\n",
    "data = response.json()\n",
    "\n",
    "# creating a data frame for data\n",
    "collisions_pd = pd.DataFrame(data)\n",
    "\n",
    "# To reduce the processing time, only keep the attribute needed\n",
    "# I have chosen the no. of the person killed, \n",
    "# and the contributing factor of the collision for further descriptive statistics measurement \n",
    "df = collisions_pd[['latitude', 'longitude', 'number_of_persons_killed', 'number_of_cyclist_killed','contributing_factor_vehicle_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf23a87-97d2-444d-bc57-bdfa86673828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the number_of_persons_killed and number_of_cyclist_killed into nnumeric\n",
    "df['number_of_persons_killed'] = pd.to_numeric(df['number_of_persons_killed'], errors='coerce')\n",
    "df['number_of_cyclist_killed'] = pd.to_numeric(df['number_of_cyclist_killed'], errors='coerce')\n",
    "\n",
    "# drop any row with NaN value in longitude/latitude\n",
    "df = df.dropna(subset=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004bc8f-af64-4177-93c7-a9e4fa3005f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Now we create a map to show the location of which collisions are recorded with death\n",
    "m = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "# Create a loop to test the value of number_of_persons_killed and number_of_cyclist_killed\n",
    "for _, row in df.iterrows():\n",
    "    persons_killed = row['number_of_persons_killed']\n",
    "    cyclists_killed = row['number_of_cyclist_killed']\n",
    "\n",
    "    if cyclists_killed > 0:  # if number_of_cyclists_killed is bigger than 0, a blue CircleMarker will be shown\n",
    "        color = 'blue'\n",
    "    elif persons_killed > 0: # if number_of_persons_killed is bigger than 0, a red CircleMarker will be shown\n",
    "        color = 'red'\n",
    "    else:\n",
    "        continue  # Nothing will happen if the row fails to meet the condition \n",
    "\n",
    "# Plotting the Map with folium map\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=1,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color, # the fill_color will be determined by the loop function\n",
    "        \n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20c09f-a6b4-4a41-8bdd-886e12ac3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the descriptive statistics for the no. of persons and cyclist killed\n",
    "mean_cyclist_killed = df_drop['number_of_cyclist_killed'].mean()\n",
    "std_cyclist_killed = df_drop['number_of_cyclist_killed'].std()\n",
    "\n",
    "mean_persons_killedd = df_drop['number_of_persons_killed'].mean()\n",
    "std_persons_killed = df_drop['number_of_persons_killed'].std()\n",
    "\n",
    "print(f\"persons_killed - Mean: {mean_persons_killedd}, Std Dev: {std_persons_killed}\")\n",
    "print(f\"cyclist_killed - Mean: {mean_cyclist_killed}, Std Dev: {std_cyclist_killed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c2701-51bf-4cab-bfee-3adbad441612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand the correlation between different contributing factors and no. of person killed\n",
    "# First create a subset to drop all rows with 'NaN' value or 'Unspecified' for contributing_factor_vehicle_1\n",
    "df_drop = df.dropna(subset=['contributing_factor_vehicle_1'])\n",
    "df_drop = df_drop[df_drop['contributing_factor_vehicle_1'] != 'Unspecified']\n",
    "\n",
    "# review the first five rows of the data\n",
    "df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eedbd2-5be3-4f0d-9492-47a8df74c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .value_counts to count the frequency of each contributing factors\n",
    "contributing_factor  = df_drop['contributing_factor_vehicle_1'].value_counts()\n",
    "\n",
    "# View the frequency of each contributing factor\n",
    "contributing_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c3702-4f16-4c62-8c50-1cea9e846130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Since there are too many contributing factors, \n",
    "# It is complicated to test them all, \n",
    "# result would also be unreliable due to the small sample size of \n",
    "# those low frequency contributing factor\n",
    "\n",
    "# So, I set up a threshold\n",
    "# If the frequency of the contributing factor is less than 0.5% of the \n",
    "# total frequency, it will be placed in 'other' \n",
    "threshold = 0.001 * contributing_factor.sum()\n",
    "\n",
    "\n",
    "major_factors = contributing_factor[contributing_factor >= threshold]\n",
    "other_factors = contributing_factor[contributing_factor < threshold].sum()\n",
    "final_factors = pd.concat([major_factors, pd.Series({'Other': other_factors})])# placing other_factors into 'other'\n",
    "\n",
    "# plotting the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "final_factors.sort_values().plot(kind='barh', color='navy', edgecolor='black')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Incidents\")\n",
    "plt.ylabel(\"Contributing Factor\")\n",
    "plt.title(\"Distribution of Contributing Factors to collision (Grouped)\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcdd9e-9b37-4a6c-bea9-09439c41dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# By using groupby function, find out the average number of persons_killed\n",
    "# for each contributing factor\n",
    "correlation = df_drop.groupby('contributing_factor_vehicle_1')['number_of_persons_killed'].mean()\n",
    "\n",
    "# Only show the factors who passed the threshold\n",
    "correlation = correlation[correlation.index.isin(final_factors.index)]\n",
    "\n",
    "# Sort the data in an ascending way\n",
    "correlation = correlation.sort_values(ascending=False)\n",
    "\n",
    "# Plotting the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlation.plot(kind='bar', color='navy', alpha=1)\n",
    "\n",
    "plt.title(\"Average Number of Persons Killed by Each Contributing Factor\")\n",
    "plt.xlabel(\"Contributing Factor\")\n",
    "plt.ylabel(\"Average Number of Persons Killed\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "#Display the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12093333-2da9-4e15-b901-0482a61ef5f7",
   "metadata": {},
   "source": [
    "## Challenge 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09204a0a-03a5-4621-9f7a-3380bae4238f",
   "metadata": {},
   "source": [
    "1. Both LongBoard and Datasher libraries are powerful for visualizing large datasets in a short amount of time. Yet, LongBoard is much flexible compared to Datasher. LongBoard allows you to create all kinds of layers, such as Solid Polygon Layer and Scatter Plot Layer. It is friendly to both vector and raster data, and the outcome of LongBoard trends is to be an interactive or customizable map. whereas, datashader works better with raster data and provides high effectiveness in processing massive data sets with million of points. It works great in handling the loss of information due to overplotting issues in data visualization. Nonetheless, I found LongBoard with more exciting functionality of its flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7a06b-c320-459a-b58c-878cc1dc8877",
   "metadata": {},
   "source": [
    "2. The original file of the dataset is a 12.8GB file with 33m rows and over 50 columns, it is a dataset of the US national-wide congestion record from 2016 to 2022. To reduce the size of the file, I created a subset with 6 columns 'Start_Lat','Start_Lng','Severity','DelayFromTypicalTraffic(mins)','DelayFromFreeFlowSpeed(mins)' and 'Congestion_Speed'] and exported the file as 'us_congestion_2016_2022_dropped', around 1.7GB. here is a onedrive link for you to access : \n",
    "https://universityofstandrews907-my.sharepoint.com/:f:/r/personal/chtu1_st-andrews_ac_uk/Documents/Data?csf=1&web=1&e=H0ujR5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b29022-e90f-4304-b5ee-9cb44ae57070",
   "metadata": {},
   "source": [
    "3. potential problem : Urban area and its outstrike trends to be Traffic Congestion hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82981042-36d8-4ea4-b511-91a91796dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I also included the code that I used to create the new file\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#file_path = '/Users/hoitik/Desktop/UA/us_congestion_2016_2022.csv'\n",
    "#df_CH4 = pd.read_csv(file_path)\n",
    "#df_CH4.head()\n",
    "\n",
    "#df_filtered = df_CH4[['Start_Lat', 'Start_Lng', 'Severity','DelayFromTypicalTraffic(mins)','DelayFromFreeFlowSpeed(mins)','Congestion_Speed']]\n",
    "#df_filtered.head()\n",
    "\n",
    "#output_path = '/Users/hoitik/Desktop/UA/us_congestion_2016_2022_dropped.csv'\n",
    "#df_filtered.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ced73-0aaa-4ff2-b55f-287456beada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = '/Users/hoitik/Desktop/UA/us_congestion_2016_2022_dropped.csv'\n",
    "df_CH4 = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55797f9b-c4c6-464f-b34d-0dc651951a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the size of the dataset\n",
    "len(df_CH4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f4a99-2cfa-4afc-be4e-d5fc2f0e0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import holoviews as hv, pandas as pd, colorcet as cc\n",
    "from holoviews.element.tiles import EsriImagery\n",
    "from holoviews.operation.datashader import datashade\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5c1d5-8667-4749-95d7-adca0d401003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader.transfer_functions as tf\n",
    "from datashader.utils import lnglat_to_meters\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# The EsriImagery was not working at the first place, \n",
    "# Then I found that the projection for the EsriImagery is mercator\n",
    "# Then I Converted the unit to meter for longitude and latitude,\n",
    "# so that It could fit the mercator projection\n",
    "df_CH4['x'], df_CH4['y'] = lnglat_to_meters(df_CH4['Start_Lng'], df_CH4['Start_Lat'])\n",
    "\n",
    "#ploting the map\n",
    "map_tiles = EsriImagery().opts(alpha=0.5, width=900, height=480, bgcolor='black')\n",
    "points = hv.Points(df_CH4, ['x', 'y'])\n",
    "Con = datashade(points, cmap=cc.fire, width=900, height=480)\n",
    "map_tiles * Con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3c4c7-ec84-4d58-a408-7e7abce6d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "map_tiles = EsriImagery().opts(alpha=0.5, width=900, height=480, bgcolor='black')\n",
    "plot = df_CH4.hvplot(\n",
    "    'x', 'y',\n",
    "    kind='scatter',\n",
    "    rasterize=True,\n",
    "    cmap=cc.fire,\n",
    "    cnorm='eq_hist'\n",
    ")\n",
    "map_tiles * plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741259b-c36e-4940-b031-bda6ce55ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "\n",
    "hv.extension('bokeh')\n",
    "map_tiles = EsriImagery().opts(alpha=0.5, width=900, height=480, bgcolor='black')\n",
    "\n",
    "\n",
    "colors = {'Slow': 'red', 'Moderate': 'yellow', 'Fast': 'green'}\n",
    "alpha_value = 0.1\n",
    "\n",
    "layers = []\n",
    "\n",
    "for speed, color in colors.items():\n",
    "    df_subset = df_CH4[df_CH4['Congestion_Speed'] == speed]\n",
    "    points = hv.Points(df_subset, ['x', 'y'])\n",
    "    shaded = datashade(points, cmap=[color], width=900, height=480)\n",
    "    layers.append(shaded)\n",
    "\n",
    "\n",
    "Con =  layers[0] *layers[1] * layers[2]\n",
    "\n",
    "\n",
    "map_tiles * Con\n",
    "\n",
    "# Congestion speed record the speed during the congestion and classify into three group\n",
    "# each group is given a colour\n",
    "# If you zoom in on the map, most urban areas are dominant by yellow (Moderate) and red (Slow)\n",
    "# Besides, the map also shows that the mid-west part of the US is also dominant by red\n",
    "# showing the severity of congestion in these area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e900068-088d-483d-8ec3-7d9f2f90b60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
