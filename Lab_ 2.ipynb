{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62dc34ff-eff8-4a3b-9aa0-2cee8c82cb92",
   "metadata": {},
   "source": [
    "# Lab No 2: Data Manipulation and Working with Web Services: 2 Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280315f-a4d4-482d-bc89-fd52c9d28506",
   "metadata": {},
   "source": [
    "## Challenge No 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c09e7f-c362-4102-a4dd-554a4fb45104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the total number of rows in the data frame\n",
    "Total_num_rows = 100\n",
    "\n",
    "# Define the value of cities, which will be used to create a loop\n",
    "cities = [\"New York\", \"London\", \"Tokyo\"]\n",
    "\n",
    "# defining the value in the table\n",
    "data = {\n",
    "    \n",
    "    # ID no for each row, ranged from 0 to the total number of rows\n",
    "    \"ID\": list(range(0, Total_num_rows)), \n",
    "    \n",
    "    # A loop of 'cities'\n",
    "    \"City\": [cities[i % len(cities)] for i in range(Total_num_rows)], \n",
    "    \n",
    "    # Random value ranged from 18 to 65 as the age \n",
    "    \"Age\": np.random.randint(18, 65, Total_num_rows), \n",
    "    \n",
    "    # Random value ranged from 0 to 999999 as the salary\n",
    "    \"Salary(USD)\": np.random.randint(0, 99999, Total_num_rows)  \n",
    "}\n",
    "\n",
    "# framing the data frame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first five row of the data frame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59eb5e9-66b9-4cf3-8077-a5df2ab064c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset from the original data frame \n",
    "subset_df = df.iloc[range(30),range(3)] #keep the frist 30 rows and 3 column\n",
    "\n",
    "#Display the subset\n",
    "print(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dddc0f-805a-44ad-8129-7df26bf1b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a filtered subset from the original data frame with loc fuction\n",
    "filtered_df = df.loc[\n",
    "         (df[\"Age\"] > 30) & # keep the data with 'age' geater than 30\n",
    "         (df[\"City\"] == \"New York\")] # keep the data with 'city' equal 'New york'\n",
    "\n",
    "# Display the filtered subset\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1cc0d-676d-4596-97e4-7b95c3942ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of fundamental statistical analysis\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd443bd-37e4-4146-93cb-fdb58f5b223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean for the numeric column in the data frame\n",
    "df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780f590-9afe-4a06-ab0a-22a416692a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation for the numeric column in the data frame\n",
    "df.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47502786-5866-4f43-b90c-f18f4f1d6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by the dffierent values of City and calculate the mean\n",
    "df.groupby('City').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d9c43-074e-440b-9a6e-dc5625c659d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by the dffierent values of City and calculate the standard deviatio\n",
    "df.groupby('City').std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a696d-028a-4a6f-a222-6d648dd55945",
   "metadata": {},
   "source": [
    "## Challenge No 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4d44a-1c6c-4056-b5fe-af7870f73f80",
   "metadata": {},
   "source": [
    "**Part No 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a190aa5-d23e-434c-b4b7-9292fc92078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Allow us to access data via API\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "#define the API url\n",
    "url_bikes = \"https://api.glasgow.gov.uk/mobility/v1/get_rentals?startDate=2022-05-01&endDate=2023-05-01\"\n",
    "\n",
    "# Requests the data with the fuction requests.get\n",
    "response = requests.get(url_bikes)\n",
    "\n",
    "# Convert the data into json format\n",
    "data = response.json()\n",
    "\n",
    "# Clear the data and create data frame\n",
    "rental_data = data['data']\n",
    "rental_pd = pd.DataFrame(rental_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3fb77-de54-4e7b-8c23-e9b63a7bffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for NaN values in the latitude and longitude column of the data frame\n",
    "nan_in_column_Lat = rental_pd['endPlaceLat'].isna().any()\n",
    "nan_in_column_Long = rental_pd['endPlaceLong'].isna().any()\n",
    "\n",
    "# Display the result\n",
    "print(nan_in_column_Lat,nan_in_column_Lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e951ad-d5b4-4c16-9671-4ebd9666be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are NaN values, we have to remove it by the .dropna fuction\n",
    "clean_rental_pd = rental_pd.dropna(subset=[\n",
    "    'endPlaceLat','endPlaceLong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2461f9-5c42-4fc1-b9a4-692b48bd20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a Geo Data Frame \n",
    "# with 'endPlaceLong' as the longtitude and 'endPlaceLat' as the latitude\n",
    "gdf_bikes_end = gpd.GeoDataFrame(clean_rental_pd, \n",
    "                                 geometry=gpd.points_from_xy(\n",
    "                                     clean_rental_pd['endPlaceLong'], \n",
    "                                     clean_rental_pd['endPlaceLat']))\n",
    "\n",
    "# Ensure that the CRS of the Geo Data Frame is EPSG:4326\n",
    "gdf_bikes_end = gdf_bikes_end.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# Display the explore map\n",
    "gdf_bikes_end.explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08e933-7f94-467a-b637-c61516ab6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now clean up the unnecessary attributes by creating a subset\n",
    "keep_cols = [\n",
    "    \"endDate\",\n",
    "    \"endPlaceId\",\n",
    "    \"endPlaceName\",\n",
    "    \"durationSeconds\",\n",
    "    \"isInvalid\",\n",
    "    \"price\",\n",
    "    \"isEbike\",\n",
    "    \"endPlaceLat\",\n",
    "    \"endPlaceLong\",\n",
    "    \"geometry\",\n",
    "]\n",
    "gdf_bikes_end = gdf_bikes_end[keep_cols]\n",
    "\n",
    "# Ensuring and chanaging data types\n",
    "gdf_bikes_end.endPlaceId = gdf_bikes_end.endPlaceId.astype(int)\n",
    "gdf_bikes_end.endPlaceName = gdf_bikes_end.endPlaceName.astype(str)\n",
    "gdf_bikes_end['endDate'] = pd.to_datetime(gdf_bikes_end['endDate'], format='%Y-%m-%dT%H:%M:%SZ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8cab3c-550e-4a71-9cc7-e601777ee274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# setting up the number of cluster area needed\n",
    "num_clusters = 4\n",
    "kmeans_collection = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "\n",
    "# calculate the cluster value with 'endPlaceLong', 'endPlaceLat' for the geo data frame\n",
    "gdf_bikes_end['kmeans_cluster'] = kmeans_collection.fit_predict(gdf_bikes_end[\n",
    "                                                                ['endPlaceLong', 'endPlaceLat']])\n",
    "\n",
    "\n",
    "# Display the first five row in the data frame to check if the kmeans_cluster column\n",
    "gdf_bikes_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95170c-fa1e-4b91-90c8-b15214c9df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the kmeans_cluster have four value\n",
    "gdf_bikes_end['kmeans_cluster'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee521d0-0d57-4700-8436-e0634166572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import leafmap\n",
    "\n",
    "# Create a map with leafmap, setting up the centre start point\n",
    "m = leafmap.Map(center=(55.860166, -4.257505), zoom=12)\n",
    "\n",
    "# Add the basemap in, with the map type of 'CartoDB.Positron'\n",
    "m.add_basemap(\"CartoDB.Positron\")\n",
    "\n",
    "# I was not able to run the leafmap in the jupyter notebook on my MacBook\n",
    "# So I ended up testing the code in google colab\n",
    "# The first set of code that I used is without mcolors and cmap='Set1'\n",
    "# It rendered a map with only three colors and that's why I added this set of code\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "colors = [\"red\", \"blue\", \"green\", \"purple\"] \n",
    "\n",
    "# name the customized colour camp which allows us to input it into the .add_data function later\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "\n",
    "# Add the data 'gdf_bikes_end' with the 'Kmeans_clusters' column into the map \n",
    "m.add_data(\n",
    "    gdf_bikes_end, \n",
    "    column=\"kmeans_cluster\",\n",
    "    cmap=cmap, # plug in the customized the colour\n",
    "    legend_title=\"Clusters\",\n",
    "    add_legend=True,\n",
    ")\n",
    "\n",
    "# plotting the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4b7f9-0f04-4729-8749-2f0ed4e2c28e",
   "metadata": {},
   "source": [
    "**Part No 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bb3fb-7a4f-4b70-ba7a-aa16c8e3745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "#access the sensor locations via API url\n",
    "url_sensor = 'http://api.glasgow.gov.uk/traffic/v1/movement/sites'\n",
    "response = requests.get(url_sensor)\n",
    "\n",
    "# Checking the response, code 200 means the request as satisfactory\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a1559-fc1f-48a0-87ac-25b68f1092c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to json format and create a data frame\n",
    "data = response.json()\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display data and review the data structure\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22937ae1-f930-46f4-9d62-07b8523af4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longitude, latitude and description are stored under 'from'\n",
    "# it has to be extracted from 'from'\n",
    "\n",
    "# Create a new column called 'from_description' with the value from 'description' under 'from'\n",
    "df['from_description'] = df['from'].apply(lambda x: x.get('description', '')) \n",
    "\n",
    "# Create a new column called 'from_lat' with the value from 'lat' under 'from'\n",
    "df['from_lat'] = df['from'].apply(lambda x: float(x.get('lat', 0)))\n",
    "\n",
    "# Create a new column called 'from_long' with the value from 'long' under 'from'\n",
    "df['from_long'] = df['from'].apply(lambda x: float(x.get('long', 0)))\n",
    "\n",
    "# After creating the new column, delete the original column 'to' and 'from'\n",
    "df = df.drop(columns=['from', 'to'])\n",
    "\n",
    "# Display the first five row to review the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f493781-b128-435f-9550-824a6a391791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for any NaN value in longitude and latitude\n",
    "nan_in_column_Long = df['from_long'].isna().any()\n",
    "nan_in_column_Lat = df['from_lat'].isna().any()\n",
    "\n",
    "# Display the result\n",
    "print(nan_in_column_Long,nan_in_column_Lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da76f1-952c-4976-8cbe-dded1bfef31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shape file with the provided path\n",
    "zone = gpd.read_file(\"/Users/hoitik/Desktop/Assginment_1/Lab 2/WorkplaceZones2011Scotland\")\n",
    "\n",
    "# Setting CRS to EPSG:4326\n",
    "zone = zone.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Since the shape is on a nation scale, it cost extra time to run\n",
    "# To clip out the zone in Glasgow city, \n",
    "# only keep the row with 'S12000046' in the column 'LADCD'\n",
    "zone_glasgow = zone[zone[\"LADCD\"] == \"S12000046\"]\n",
    "\n",
    "# Display the map and review\n",
    "zone_glasgow.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cef034-cc36-423b-b215-20af2dd99183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we read the zone file, \n",
    "# we now create a geo data frame with the sensors location and clip it with 'zone_glasgow'\n",
    "\n",
    "# Create Geo Data Frame with 'from_long' as the longitude and 'from_lat' as the latitude\n",
    "gdf_sensors = gpd.GeoDataFrame(df, \n",
    "                               geometry = gpd.points_from_xy(df['from_long'],\n",
    "                                                             df['from_lat']), \n",
    "                               crs = 'EPSG:4326') # with CRS EPSG:4326\n",
    "\n",
    "# Clip the sensors location with 'zone_glasgow'\n",
    "gdf_sensors = gpd.clip(gdf_sensors, zone_glasgow)\n",
    "\n",
    "# Display and review the map\n",
    "gdf_sensors.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466f081-e504-4c4f-a8fd-3ef88a4629fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two map with .sjoin\n",
    "joined = gpd.sjoin( gdf_sensors, zone_glasgow, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# display the first five row to review\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73859a7d-8e9f-4a41-813a-1e97accebe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of sentor in each zone by 'WZCD'\n",
    "# Also create a new column 'sensor_count' to store the result\n",
    "sensor_counts = joined.groupby(\"WZCD\").size().reset_index(name=\"sensor_count\")\n",
    "\n",
    "# Now we merge the new column into 'zone_glasgow'\n",
    "counts_by_zone = zone_glasgow.merge(sensor_counts, on=\"WZCD\", how=\"left\").fillna(0)\n",
    "\n",
    "# Poltting the map\n",
    "counts_by_zone.explore(\n",
    "    center=(55.860166, -4.257505),\n",
    "    zoom=12, \n",
    "    column=\"sensor_count\", \n",
    "    cmap=\"cividis\", \n",
    "    legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e1152-9535-4556-ad34-b71104d5d769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
